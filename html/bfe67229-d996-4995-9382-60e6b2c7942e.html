<!-- URL: https://en.wikipedia.org/wiki/BERT_(language_model) -->
<!DOCTYPE html>
<html>
<head>
    <title>BERT - Wikipedia</title>
    <meta charset="UTF-8">
</head>
<body>
    <h1>BERT</h1>
    
        BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique 
        for natural language processing pre-training developed by Google. BERT was created and published in 2018 by Jacob 
        Devlin and his colleagues from Google.
        
        In 2019, Google announced that it had begun leveraging BERT in its search engine, and by late 2020 it was being 
        used in almost every English-language query. BERT is designed to help computers understand the meaning of ambiguous 
        language in text by using surrounding text to establish context. The BERT framework was pre-trained using text from 
        Wikipedia and can be fine-tuned with question-and-answer datasets. BERT has significantly improved the state-of-the-art 
        in many NLP tasks including information retrieval.
        
</body>
</html>