<!-- URL: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval) -->
<!DOCTYPE html>
<html>
<head>
    <title>Evaluation Metrics - Wikipedia</title>
    <meta charset="UTF-8">
</head>
<body>
    <h1>Evaluation Metrics</h1>
    
        Evaluation is crucial in information retrieval for measuring the effectiveness of systems and algorithms. Standard evaluation 
        metrics include precision, recall, F-measure, mean average precision (MAP), normalized discounted cumulative gain (NDCG), and 
        mean reciprocal rank (MRR).
        
        Precision measures the fraction of retrieved documents that are relevant. Recall measures the fraction of relevant documents 
        that are retrieved. These metrics often trade off against each other. MAP provides a single-figure measure of quality across 
        recall levels. NDCG is particularly useful for web search where relevance is graded rather than binary. Test collections like 
        TREC provide standardized benchmarks for comparing different IR systems. Offline evaluation using test collections is complemented 
        by online evaluation using A/B testing and click-through analysis. Good evaluation methodology is essential for advancing the 
        field of information retrieval.
        
</body>
</html>